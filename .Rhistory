dtm <- DocumentTermMatrix(docs)
inspect(dtm)
mdtm <- as.matrix(dtm)
write.csv(unlist(dtm), "N_DTM_1.csv")
class(dtm)
tdm <- TermDocumentMatrix(docs)
write.csv(as.matrix(dtm),"aa_1.csv")
write.csv(mdtm,"aa2_1.csv")
inspect(tdm)
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix_1.csv")
dtms <- removeSparseTerms(dtm, 0.95)
dtms
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(wordcloud)
dtms <- removeSparseTerms(dtm, 0.9) # Prepare the data (max 15% empty space)
freq <- colSums(as.matrix(dtm))# Find word frequencies
str(freq)
class(freq)
freq_1 <- freq %>% as.data.frame() %>% arrange(desc(.)) %>% filter(rownames()%in%c("deebot", "'s", "will", "much", "'ve", "'m", " "))
freq %>% as.data.frame() %>% arrange(desc(.)) %>% head(200)
rownames(freq_1)
color <- brewer.pal(8, "Accent")
brewer.pal.info
wordcloud(names(freq), freq, max.words=150, rot.per=0.4, colors=color, random.order=F)
## crawling
library(rvest)
library(RCurl)
'setwd("C:/Users/ammy7/OneDrive/¹ÙÅÁ È­¸é/texts")
url <- "https://www.amazon.com/ECOVACS-N79S-Connectivity-Controls-Self-Charging/product-reviews/B077HW9XM7/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
N_pages <- 500
A <- NULL
for(j in 1: N_pages){
alexa <- read_html(paste0(url, j))
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
A <- rbind(A,B)
print(j)
}
print(j)
head(A,15)
tail(A,5)
write.csv(data.frame(A),"EcovacsDEEBOT.csv")
A[3, 1]
## Word Cloud
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
install.packages(Needed, dependencies = TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("C:/Users/ammy7/OneDrive/¹ÙÅÁ È­¸é/texts")
cname <- file.path("C:/Users/ammy7/OneDrive/¹ÙÅÁ È­¸é", "texts")
cname
dir(cname)'
library(tm)
x <- getURL("https://github.com/2020719512/Marketing_SKKU_Data/blob/master/ecovacs-deebot2709.csv")
data <- read.csv(text = x)
#data <- read.csv("EcovacsDEEBOT.csv")
head(data)
data<-data[,2]
head(data,10)
docs <- Corpus(VectorSource(data))
summary(docs)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("vacuum", "deebot", "'s", "don't", "doesn't", "didnt", "'m", "ive", "'s", "will", "much", "'ve", "'m", " ", "months"))
docs <- tm_map(docs, stripWhitespace)
library(tm)
install.packages("SnowballC")
library(SnowballC)
dtm <- DocumentTermMatrix(docs)
inspect(dtm[1:20,1:20])
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
mdtm <- as.matrix(dtm)
write.csv(unlist(dtm), "N_DTM_1.csv")
class(dtm)
tdm <- TermDocumentMatrix(docs)
write.csv(as.matrix(dtm),"aa_1.csv")
write.csv(mdtm,"aa2_1.csv")
inspect(tdm)
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix_1.csv")
dtms <- removeSparseTerms(dtm, 0.95)
dtms
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(wordcloud)
dtms <- removeSparseTerms(dtm, 0.9) # Prepare the data (max 15% empty space)
freq <- colSums(as.matrix(dtm))# Find word frequencies
str(freq)
class(freq)
freq_1 <- freq %>% as.data.frame() %>% arrange(desc(.)) %>% filter(rownames()%in%c("deebot", "'s", "will", "much", "'ve", "'m", " "))
freq %>% as.data.frame() %>% arrange(desc(.)) %>% head(200)
rownames(freq_1)
color <- brewer.pal(8, "Accent")
brewer.pal.info
wordcloud(names(freq), freq, max.words=150, rot.per=0.4, colors=color, random.order=F)
install.packages("SnowballC")
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
#url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
url <- " https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber= "
# 3. Specify how many pages you would like to scrape
N_pages <- 1000 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
#url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
url <- " https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber= "
# 3. Specify how many pages you would like to scrape
N_pages <- 100 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
#url <- " https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber= "
# 3. Specify how many pages you would like to scrape
N_pages <- 100 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
#url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
url <- "https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
# 3. Specify how many pages you would like to scrape
N_pages <- 100 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
#url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
url <- "https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
# 3. Specify how many pages you would like to scrape
N_pages <- 300 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
# Echoshow8 example
# 2020/9/14
# product review and review date
# Review carefully 'for' loop and cbind and rbind. 'for' loop enables you to collect multiple pages of reviews
# cbind, column bind, makes you to have nice a table of reviews which can be easily analyzed by statistical approach such as regression.
# http://www.endmemo.com/program/R/rbind.php
# 1. Install two packages
# install.packages('rvest')
# install.packages('RCurl')
library(rvest)
library(RCurl)
# 2. Identify the web address
# search product - click reviews - sort by recent
# remove the page number at the end of the URL
#url <- "https://www.amazon.com/Charcoal-Kitchen-Network-Complimentary-Subscription/product-reviews/B086ZF1T9X/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
url <- "https://www.amazon.com/iRobot-Roomba-675-Connectivity-Carpets/product-reviews/B07DL4QY5V/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber="
# 3. Specify how many pages you would like to scrape
N_pages <- 300 # It would be easier to test with small number of pages and it depends on your own source website
A <- NULL
for (j in 1: N_pages)
{
alexa <- read_html(paste0(url, j))
#help paste: http://www.cookbook-r.com/Strings/Creating_strings_from_variables/
B <- cbind(alexa %>%
html_nodes(".review-text") %>%
html_text(), alexa %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text()     )
# I replaced html_nodes for review date with "#cm_cr-review_list .review-date"
# "#cm_cr-review_list" makes two irrelevant parts for the top positive/critical reviews and '#'is the magic sign for unselect the part
A <- rbind(A,B)
}
# 4. Make sure what you got
print(j) # This command shows the progress of the for loop. This example it means number of pages.
#A[,1] # this will print the first column of your output
# 4.1 Another way to double-check
head(A,15)
tail(A,5)
# 5. Save the output
write.csv(data.frame(A),"irobot.csv") #you can change the file name
## crawling
library(rvest)
library(RCurl)
library(tm)
library(SnowballC)
data <- read.csv("https://raw.githubusercontent.com/2020719512/Marketing_SKKU_Data/master/irobot.csv")
stpwords <- read.csv("https://raw.githubusercontent.com/2020719512/Marketing_SKKU_Data/master/stop_words.txt")
stpwords <- as.vector(unlist(stpwords)) #convert data frame to vector
data<-data[,2]
docs <- Corpus(VectorSource(data))
summary(docs)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stpwords)
docs <- tm_map(docs, removeWords, c("its","ive","robot","roomba", "vacuuming","vacuum", "deebot", "ecovacs", "ve", "'s", "don't", "doesn't", "'m", "ive", "'s", "'ve", "'m"))
docs <- tm_map(docs, stripWhitespace)
#docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
inspect(dtm[1:20,1:20])
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
mdtm <- as.matrix(dtm)
#write.csv(unlist(dtm), "N_DTM_1.csv")
class(dtm)
tdm <- TermDocumentMatrix(docs)
#write.csv(as.matrix(dtm),"aa_1.csv")
#write.csv(mdtm,"aa2_1.csv")
inspect(tdm)
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
#write.csv(m, file="DocumentTermMatrix_1.csv")
dtms <- removeSparseTerms(dtm, 0.95)
dtms
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
------------------------------------------------------------
#word frequency chart
library(ggplot2)
df <- data.frame(stringsAsFactors=FALSE,
word = head(wf$word, 15),
freq = head(wf$freq, 15)
)
ggplot(df, aes(x = reorder(word, freq), y = freq)) +
geom_col() +
labs(title="Word Frequency Chart ",
x = NULL,
y = "Frequency") +
coord_flip()
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq > 400), aes(x = reorder(word, -freq), y = freq)) +   labs(title="Word Frequency Chart ",
x = NULL,
y = "Frequency")
# You can modify 300 into your own number for best output
p <- p + geom_bar(stat="identity")+ theme(axis.text.x=element_text(angle=45, hjust=1))
p
-------------------------------------------------------------------------
library(wordcloud)
dtms <- removeSparseTerms(dtm, 0.9) # Prepare the data (max 15% empty space)
freq <- colSums(as.matrix(dtm))# Find word frequencies
str(freq)
class(freq)
freq_1 <- freq %>% as.data.frame() %>% arrange(desc(.)) %>% filter(rownames()%in%c("deebot", "'s", "will", "much", "'ve", "'m", " "))
freq %>% as.data.frame() %>% arrange(desc(.)) %>% head(200)
rownames(freq_1)
color <- brewer.pal(8, "Accent")
color <- c("#7FC97F", "#BEAED4" ,"#FDC086" ,"#87D4A1" ,"#386CB0", "#F0027F", "#BF5B17","#666666")
brewer.pal.info
wordcloud(names(freq), freq, max.words=150, rot.per=0.4, colors=color, random.order=F)
## crawling
library(rvest)
library(RCurl)
library(tm)
library(SnowballC)
data <- read.csv("https://raw.githubusercontent.com/2020719512/Marketing_SKKU_Data/master/irobot.csv")
stpwords <- read.csv("https://raw.githubusercontent.com/2020719512/Marketing_SKKU_Data/master/stop_words.txt")
stpwords <- as.vector(unlist(stpwords)) #convert data frame to vector
data<-data[,2]
docs <- Corpus(VectorSource(data))
summary(docs)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stpwords)
docs <- tm_map(docs, removeWords, c("'t","irobot","'s","its","ive","robot","roomba", "vacuuming","vacuum", "deebot", "ecovacs", "ve", "'s", "don't", "doesn't", "'m", "ive", "'s", "'ve", "'m"))
docs <- tm_map(docs, stripWhitespace)
#docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(docs)
inspect(dtm[1:20,1:20])
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
mdtm <- as.matrix(dtm)
#write.csv(unlist(dtm), "N_DTM_1.csv")
class(dtm)
tdm <- TermDocumentMatrix(docs)
#write.csv(as.matrix(dtm),"aa_1.csv")
#write.csv(mdtm,"aa2_1.csv")
inspect(tdm)
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
#write.csv(m, file="DocumentTermMatrix_1.csv")
dtms <- removeSparseTerms(dtm, 0.95)
dtms
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
------------------------------------------------------------
#word frequency chart
library(ggplot2)
df <- data.frame(stringsAsFactors=FALSE,
word = head(wf$word, 15),
freq = head(wf$freq, 15)
)
ggplot(df, aes(x = reorder(word, freq), y = freq)) +
geom_col() +
labs(title="Word Frequency Chart ",
x = NULL,
y = "Frequency") +
coord_flip()
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq > 400), aes(x = reorder(word, -freq), y = freq)) +   labs(title="Word Frequency Chart ",
x = NULL,
y = "Frequency")
# You can modify 300 into your own number for best output
p <- p + geom_bar(stat="identity")+ theme(axis.text.x=element_text(angle=45, hjust=1))
p
-------------------------------------------------------------------------
library(wordcloud)
dtms <- removeSparseTerms(dtm, 0.9) # Prepare the data (max 15% empty space)
freq <- colSums(as.matrix(dtm))# Find word frequencies
str(freq)
class(freq)
freq_1 <- freq %>% as.data.frame() %>% arrange(desc(.)) %>% filter(rownames()%in%c("deebot", "'s", "will", "much", "'ve", "'m", " "))
freq %>% as.data.frame() %>% arrange(desc(.)) %>% head(200)
rownames(freq_1)
color <- brewer.pal(8, "Accent")
color <- c("#7FC97F", "#BEAED4" ,"#FDC086" ,"#87D4A1" ,"#386CB0", "#F0027F", "#BF5B17","#666666")
brewer.pal.info
wordcloud(names(freq), freq, max.words=150, rot.per=0.4, colors=color, random.order=F)
